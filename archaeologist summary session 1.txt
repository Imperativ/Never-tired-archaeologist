# Chat-History: Never-Tired-Archaeologist – Dokumenten-Organizer
**Projekt-Übersicht:** Privatprojekt zur Organisation von AI-Dokumenten mit Google Document AI Warehouse API. Codenames: IndianaJones (Analyse), HenryJones (Organisieren). GCP-Projekt: the-never-tired-archaeologist. Privatnutzung: Minimale Sicherheit, API-Keys als normale Files behandeln (statische Analyse, keine Exec).

**Datum des Exports:** [Heutiges Datum, z.B. 2023-10-XX – ersetze bei Bedarf]. Vollständiger Verlauf in sequentiellen Runden.

## Runde 1: Problemstellung (User)
Ein Ordner mit 50+ Files (TXT, MD, PDF, Code) aus AI-Projekten der letzten 12 Monate – kreuz und quer (Prompts, Outputs, Docs). Ziel: App/Script, das rekursiv scannt, Metadaten extrahiert via Document AI Warehouse API (https://cloud.google.com/document-warehouse/docs/overview), Ordnerstruktur erstellt und Files verschiebt. Vorgeschlagene Metadaten: language, topic, date_extracted, keywords, confidence_score, git project, prompt (y/n), Wortanzahl, Erstelldatum. App soll dynamisch neue Metadaten erkennen und Unterordner anlegen. Frage: Ideen zur Umsetzung? Zuerst analysieren, dann organisieren?

## AI-Antwort 1: Vorschläge und Starter-Script
- Zustimmung zu 2-Phasen-Modell (Analyse > Organisieren).
- Erweiterte Metadaten: author/source, entities, summary, file_type_detected, related_files (dynamisch via API).
- Architektur: Python-CLI-Script (z.B. mit google-cloud-contentwarehouse, pdfplumber), GCP-Setup (Service Account, Schema).
- Herausforderungen: Große Files (Komprimieren), Code statisch analysieren (keine Exec).
- Beispiel-Script: Einzelscript `organize_docs.py` mit Phase1 (Metadaten-Extract), Phase2 (Verschieben), argparse, Fallbacks.

Code-Beispiel:
```python
# [Full script from first response, including imports, extract_metadata, phase1_analyze, phase2_organize]
